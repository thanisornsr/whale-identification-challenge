{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take home test _ Thanisorn Sriudomporn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is provided on https://www.kaggle.com/c/whale-categorization-playground/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Briefly introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test, the objective is to match the humpback whale with their ID by it tail. The Images and Ids are giving in this data set. The mainly challenge here happened when looking at the data, which will be demonstrated later. There are only few training example for each of 4000+ Ids. In this jupyter notebook, the step by step of training algorithm to reach the objective will be shown and described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the basic libraries are imported, such as numpy, pandas, opencv, os. Then, one import keras to train and validate model as a main framework one will working on from now. The sklearn is used here to give the training output (Ytrain) as a integer type. To manage the data file, one choose shutil in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Activation, Dense, Lambda, Flatten, Embedding, PReLU, BatchNormalization, GlobalAveragePooling2D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data manipulating and Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before working with the data set, one randomly pick image to see and get dimensions using Opencv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(391, 682, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load Image\n",
    "img = cv2.imread('./Images/train/train/00d6b82e.jpg')\n",
    "\n",
    "# Show Image\n",
    "cv2.imshow('image', img)\n",
    "\n",
    "# Get the dimension and print out\n",
    "dimensions= img.shape\n",
    "print(dimensions)\n",
    "\n",
    "# wait for the key to continue\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, one create variables to store the path where one want to split and store datas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory of Train and Test Images\n",
    "raw_train_dir = './Images/train/train/'\n",
    "#Directory of Train, validade, Test Image\n",
    "train_dir = './Images/original/train/'\n",
    "validate_dir = './Images/original/validade/'\n",
    "test_dir = './Images/original/test/'\n",
    "\n",
    "#Directory for Processed Trainm validade\n",
    "p_train_dir = './Images/processed/train/'\n",
    "p_validate_dir = './Images/processed/validade/'\n",
    "p_test_dir = './Images/processed/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The step which is worth mentioned here is to take a look on the datas to get roughly idea about the procedure one is going to use. One use Pandas to read .csv file as DataFrame type. Then, the Ids are counted and printed out to see what is going on with this data set. As one can see, there is a big difference of image number between the 'new_whale' and some of Ids that have only 1 image. In this step, one realize that the data is not enough to train the accurate model straightforward. Then, one extract the Ids and images into array. After that, the Ids array are turn into one hot encoding and the number of Id are printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the number of datas\n",
      "           Image\n",
      "Id              \n",
      "new_whale    810\n",
      "w_1287fbc     34\n",
      "w_98baff9     27\n",
      "w_7554f44     26\n",
      "w_1eafe46     23\n",
      "...          ...\n",
      "w_7e48479      1\n",
      "w_7e728d8      1\n",
      "w_7e8305f      1\n",
      "w_7e841fa      1\n",
      "w_ffdab7a      1\n",
      "\n",
      "[4251 rows x 1 columns]\n",
      "Total Number of Class : 4251\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = './label/train.csv'\n",
    "\n",
    "# read .csv as DataFrame\n",
    "train_datas = pd.read_csv(train_data_dir)\n",
    "\n",
    "# Count the number of image for each Ids\n",
    "id_count = (train_datas.groupby('Id').count()).sort_values('Image',ascending = False)\n",
    "print('Checking the number of datas')\n",
    "print(id_count)\n",
    "\n",
    "# Store Ids and Image's names\n",
    "id_array_value = train_datas['Id'].values\n",
    "image_array = train_datas['Image'].values\n",
    "\n",
    "# Convert the Ids to One hot encoding\n",
    "onehot = pd.get_dummies(id_array_value)\n",
    "id_onehot = onehot.values\n",
    "\n",
    "# Get the number of Ids\n",
    "nclass = id_onehot.shape[1]\n",
    "print('Total Number of Class : ' + str(nclass))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, one use the preprocesser from sklearn to create the Ids from string to integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.fit(id_array_value)\n",
    "id_array_value_int = le.transform(id_array_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One go through all image in train folder. In this project, one split the image in train folder into 3 parts which are training images, validation images ,and testing images. The reason one including testing image in this part because in the actual testing images in test folder are not provided with Ids. Since it is the modelling problem, one need the result to verify the model. The ratio between training, validation, and testing images are set as 70%, 15%, and 15% respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of total training images\n",
    "list = os.listdir(raw_train_dir)\n",
    "nbr_train = len(list)\n",
    "\n",
    "# Set the ratios of train/validate/test to 75%/15%/15%\n",
    "train_ratio = 0.70\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# get the number of validation and testing images\n",
    "nbr_validate = round(val_ratio*nbr_train)\n",
    "nbr_test = round(test_ratio*nbr_train)\n",
    "\n",
    "# Random indexes of validation and testing images\n",
    "index_random = np.random.choice(nbr_train,nbr_validate+nbr_test)\n",
    "index_validate = index_random[0:nbr_validate-1]\n",
    "index_test = index_random[nbr_validate:]\n",
    "index_validate_list = index_validate.tolist()\n",
    "index_test_list = index_test.tolist()\n",
    "\n",
    "# Go through all images in train folder and split some of them into validation folder and test folder using the randomed indexes.\n",
    "index_temp = 1\n",
    "for filename in os.listdir(raw_train_dir):\n",
    "    if index_temp in index_validate_list:\n",
    "        shutil.copy2(raw_train_dir+filename,validate_dir+filename)\n",
    "    elif index_temp in index_test_list:\n",
    "        shutil.copy2(raw_train_dir+filename,test_dir+filename)\n",
    "    else:\n",
    "        shutil.copy2(raw_train_dir+filename,train_dir+filename)\n",
    "    index_temp = index_temp+1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use images as training input, one make a function here to resize image into the selected size related to the model selected. After look carefully in those images in train folder, one notice that there are images with explaination text in the bottom of the image, which is not going to be beneficial to the model. One use Opencv to do the image processing in order to get rid of those text, i.e. crop it out. The roughly procedure in crop_text_out function is started with convert image into grayscale and blur it. Then, fill the top 70% of image with black and do the thresholding on filled image. Then one use Morphological Transformations to make the thresholded area a little bit easier to contour and see that if the area one detected in this mask, i.e. the white area around the text, is bigger than 15% of image's area. One crop it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shrink_img(img,sizex,sizey):\n",
    "    dsize = (sizex,sizey)\n",
    "    img_shrink = cv2.resize(img,dsize)\n",
    "    return img_shrink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_text_out(img):\n",
    "    # Convert image into gray scale and blur it\n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_gray = cv2.blur(img_gray, (5,5))\n",
    "    (h,w) = img_gray.shape\n",
    "    \n",
    "    # fill the top 70% of image with black \n",
    "    y_to_crop = int(round(0.7*h))\n",
    "    img_gray = cv2.rectangle(img_gray,(0,0),(w,y_to_crop),(0,0,0),-1)\n",
    "    \n",
    "    # Threadhold the filled image to get white\n",
    "    ret, thresh = cv2.threshold(img_gray, 200, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    # Morphological Transformations\n",
    "    rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(2,2))\n",
    "    thresh = cv2.morphologyEx(thresh,cv2.MORPH_CLOSE, rect_kernel)\n",
    "    \n",
    "    # Find contours\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # crop the image if the detected contour is bigger than 15% of image\n",
    "    got_one = 0\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) >= 0.15 * w * h:\n",
    "            xt,yt,wt,ht = cv2.boundingRect(cnt)\n",
    "            got_one = 1\n",
    "    if got_one:\n",
    "        img_crop = img[0:yt,0:w,:]\n",
    "    else:\n",
    "        img_crop = img\n",
    "\n",
    "    return img_crop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, one process through training, validation, and testing images with crop and resize function. Then, the images are saved into processed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = [train_dir,validate_dir,test_dir]\n",
    "target_dirs = [p_train_dir,p_validate_dir,p_test_dir]\n",
    "sizex = 224 \n",
    "sizey = 224 \n",
    "for i in range(0,3):\n",
    "    dir_temp = dirs[i]\n",
    "    target_temp = target_dirs[i]\n",
    "    for filename in os.listdir(dir_temp):\n",
    "        img = cv2.imread(dir_temp+filename)\n",
    "        img_crop = crop_text_out(img)\n",
    "        img_shrink = shrink_img(img_crop,sizex,sizey)\n",
    "        cv2.imwrite(target_temp+filename,img_shrink)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stack_from_dir function, the directory of images in the argument or function. One read the image in the directory and stack it up in one array called Xstack. For Ystack, one use the filename, i.e. image name, to look into the image array to get the index and use that index to get the one hot encoding. The same index is also use to get the Id in the integer form from Id_attay_value_int."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_from_dir(dir):\n",
    "    Xstack = []\n",
    "    Ystack = []\n",
    "    Ystack_value = []\n",
    "    for filename in os.listdir(dir):\n",
    "        img = cv2.imread(dir+filename)\n",
    "        \n",
    "        # stack the image to Xstack\n",
    "        Xstack.append(img)\n",
    "        \n",
    "        # get one hot encoding\n",
    "        id_temp = id_onehot[image_array == filename]\n",
    "        \n",
    "        # get Id in integer format\n",
    "        id_value = id_array_value_int[image_array == filename]\n",
    "        \n",
    "        # stack those y to Ystack and Ystack_value\n",
    "        Ystack.append(id_temp.T)\n",
    "        Ystack_value.append(id_value.T)\n",
    "    \n",
    "    Xstack = np.asarray(Xstack)\n",
    "    Ystack = np.asarray(Ystack)\n",
    "    Ystack = Ystack[:,:,0]\n",
    "    Ystack_value = np.asarray(Ystack_value)\n",
    "    \n",
    "    return [Xstack,Ystack,Ystack_value];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Training shape: (7285, 224, 224, 3)\n",
      "Y Training shape: (7285, 4251)\n",
      "Number of train images: 7285\n",
      "X Validate shape: (1367, 224, 224, 3)\n",
      "Y Validate shape: (1367, 4251)\n",
      "Number of validate images: 1367\n",
      "X Test shape: (1198, 224, 224, 3)\n",
      "Y Test shape: (1198, 4251)\n",
      "Number of test images: 1198\n"
     ]
    }
   ],
   "source": [
    "# stack arrays from 3 directories defined in previous\n",
    "[Xtrain,Ytrain, Ytrain_value] = stack_from_dir(p_train_dir)\n",
    "[Xval , Yval,Yval_value] = stack_from_dir(p_validate_dir)\n",
    "[Xtest , Ytest,Ytest_value] = stack_from_dir(p_test_dir)\n",
    "\n",
    "# Normalize image vector\n",
    "Xtrain = Xtrain/255.\n",
    "Xval = Xval/255.\n",
    "Xtest = Xtest/255.\n",
    "\n",
    "# Get the exact number of training, validation and testing image\n",
    "nTrain = Xtrain.shape[0]\n",
    "nVal = Xval.shape[0]\n",
    "nTest = Xtest.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "# Let print some values\n",
    "print('X Training shape: ' + str(Xtrain.shape))\n",
    "print('Y Training shape: ' + str(Ytrain.shape))\n",
    "print('Number of train images: ' + str(nTrain))\n",
    "print('X Validate shape: ' + str(Xval.shape))\n",
    "print('Y Validate shape: ' + str(Yval.shape))\n",
    "print('Number of validate images: ' + str(nVal))\n",
    "print('X Test shape: ' + str(Xtest.shape))\n",
    "print('Y Test shape: ' + str(Ytest.shape))\n",
    "print('Number of test images: ' + str(nTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>When it come to modelling time, one have to consider the model to work on. There are difficulties in choosing model since most of them have their own advantages and disadvantages. One way to do this is to research and try many kinds of model. After test and evaluate it a little bit, one might have some idea to go on or change the model.</p>\n",
    "\n",
    "<p>In this task, I decided to use the pre-trained model without the top to be my base model since pre-trained model come with pre-trained weight which might make my training faster. The base model's parameters can be set as trainable and non-trainable. To make it trainable, one need to train it on powerful resource because of the much more parameters to train.  </p>\n",
    "\n",
    "<p>I decided to do the multitask learning with 2 inputs and 2 outputs here because I also want to use the center loss in this task. The first input and output is the image and Id (in one hot encoding form). Another is the Ids in integer form and some dummy array in the same size. The reason, why I selected center-loss, is that center-loss is one of the popular method for face recognition. There are another options such as Siamese Neural network or Triplet loss to try out. Anyway, I chose center-loss to perform in this task. The loss of both softmax and center will be considered. One issue here is that the center-loss layer use the output y1 as it input to compute loss. In the future, one can maybe use x instead of y2. This is the experimental work, i.e. it is to trial and error.</p>\n",
    "The network flow is shown in the image here >> https://drive.google.com/open?id=1SRqULMGk2jeUuf558yBZG-bcIRcuLgQV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "def first_model(input_shape1,input_shape2, nclass, dropout, learning_rate = 0.001):\n",
    "    \n",
    "    # Use ResNet50 as a base model to start this model\n",
    "    base_model = ResNet50(weights = 'imagenet', include_top=False)\n",
    "    \n",
    "    # One can choose to set the base model's parameters as trainable or non-trainable.\n",
    "    # In this case, I set as trainable.\n",
    "    \n",
    "#     for layer in base_model.layers[:]:\n",
    "#         layer.trainable = False\n",
    "    \n",
    "    # pass input 1 through base model, fully connected and dropout\n",
    "    model_input_1 = Input(input_shape1)\n",
    "    x = base_model(model_input_1)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    y1 = Dense(1024,activation='relu')(x)\n",
    "    y1 = Dropout(dropout)(y1)\n",
    "    y1 = Dense(1024,activation='relu')(y1)\n",
    "    y1 = Dropout(dropout)(y1)\n",
    "    \n",
    "    y1_h = Dense(nclass, activation = 'softmax', name = 'Id')(y1)\n",
    "    \n",
    "    \n",
    "    # Here one start from the second output (Center loss) using the y1 and the input 2\n",
    "    model_input_2 = Input(input_shape2)\n",
    "    centers = Embedding(nclass,1024)(model_input_2) \n",
    "    # This l2_loss is center-loss\n",
    "    l2_loss = Lambda(lambda x: K.sum(K.square(x[0]-x[1][:,0]),1,keepdims=True),name='l2_loss')([y1,centers])\n",
    "    \n",
    "    # Define mofel\n",
    "    model = Model(inputs=[model_input_1,model_input_2], outputs= [y1_h,l2_loss])\n",
    "    \n",
    "    \n",
    "    # set the optimizer as Adam optimizer\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    # Complie model\n",
    "    model.compile(loss=['categorical_crossentropy',lambda y_true,y_pred:y_pred], optimizer=optimizer,loss_weights=[1,0.4],metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step one create, i.e. compile, model with training image shape and the Y shape. The dropout is set as 0.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thani\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resnet50 (Model)                multiple             23587712    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           resnet50[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1024)         0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1024)         1049600     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 1024)      4353024     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Id (Dense)                      (None, 4251)         4357275     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "l2_loss (Lambda)                (None, 1)            0           dropout_2[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 35,445,787\n",
      "Trainable params: 35,392,667\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = first_model(input_shape1 = (224,224,3), input_shape2 = (1,), nclass = nclass, dropout = 0.2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue worth mentioned here is the **data augmentation**. One realize that the number of images for each Ids are relatively small. One use data augmentation to get more training image using some technique, such as rotation, flip, or shift. The ImageDataGenerator in the generator that help us generate the augmented data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generate_data_generator is use to be an object that generate inputs and outputs to the fitting function since one have 2 inputs and 2 outputs in the model and one of the input is better to do the augmentation before training. The batch size is set as 8 since I run it on my personal laptop and this is the maximum the memory can go for. This function is to work with model.fit_generator(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_generator(generator, Xtrain, Ytrain):\n",
    "    # data_gen as the data generator\n",
    "    data_gen = generator.flow(Xtrain, Ytrain,batch_size = 8)\n",
    "    while True:\n",
    "        # get generated data from data_gen\n",
    "        Xtrain_gen, Ytrain_gen = data_gen.next()\n",
    "        # covert the one hot encoding Ytrain_gen to be a integer form\n",
    "        Ytrain_int = np.where(Ytrain_gen == 1)[1]\n",
    "        # just another dummy here\n",
    "        Ytrain_dummy = np.random.rand(Ytrain_int.shape[0],1)\n",
    "\n",
    "        yield [Xtrain_gen, Ytrain_int], [Ytrain_gen,Ytrain_dummy]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is set to early stop when the loss in not decrease to prevent model from being overfit after 5 epochs and the check point is to save the best model found in the training. Steps per epoch are set related to the size as 100. Here one use [Xval,Yval_value], [Yval,Yval_dummy] to validate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "100/100 [==============================] - 115s 1s/step - loss: 8.1579 - Id_loss: 8.1181 - l2_loss_loss: 0.0994 - Id_accuracy: 0.0887 - l2_loss_accuracy: 0.0000e+00 - val_loss: 8.4462 - val_Id_loss: 8.3287 - val_l2_loss_loss: 0.2935 - val_Id_accuracy: 0.0863 - val_l2_loss_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00001: saving model to best_model.h5\n",
      "Epoch 2/20\n",
      "100/100 [==============================] - 72s 724ms/step - loss: 8.1492 - Id_loss: 8.1105 - l2_loss_loss: 0.0968 - Id_accuracy: 0.0950 - l2_loss_accuracy: 0.0000e+00 - val_loss: 8.4495 - val_Id_loss: 8.3345 - val_l2_loss_loss: 0.2870 - val_Id_accuracy: 0.0863 - val_l2_loss_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00002: saving model to best_model.h5\n",
      "Epoch 3/20\n",
      "100/100 [==============================] - 72s 722ms/step - loss: 8.1752 - Id_loss: 8.1383 - l2_loss_loss: 0.0924 - Id_accuracy: 0.0712 - l2_loss_accuracy: 0.0000e+00 - val_loss: 8.4541 - val_Id_loss: 8.3411 - val_l2_loss_loss: 0.2822 - val_Id_accuracy: 0.0863 - val_l2_loss_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00003: saving model to best_model.h5\n",
      "Epoch 4/20\n",
      "100/100 [==============================] - 73s 729ms/step - loss: 8.1084 - Id_loss: 8.0718 - l2_loss_loss: 0.0915 - Id_accuracy: 0.0875 - l2_loss_accuracy: 0.0000e+00 - val_loss: 8.4579 - val_Id_loss: 8.3473 - val_l2_loss_loss: 0.2763 - val_Id_accuracy: 0.0863 - val_l2_loss_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00004: saving model to best_model.h5\n",
      "Epoch 5/20\n",
      "100/100 [==============================] - 73s 726ms/step - loss: 8.1248 - Id_loss: 8.0866 - l2_loss_loss: 0.0953 - Id_accuracy: 0.0763 - l2_loss_accuracy: 0.0000e+00 - val_loss: 8.4616 - val_Id_loss: 8.3535 - val_l2_loss_loss: 0.2699 - val_Id_accuracy: 0.0863 - val_l2_loss_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00005: saving model to best_model.h5\n",
      "Epoch 6/20\n",
      "100/100 [==============================] - 73s 726ms/step - loss: 8.1000 - Id_loss: 8.0625 - l2_loss_loss: 0.0936 - Id_accuracy: 0.0875 - l2_loss_accuracy: 0.0000e+00 - val_loss: 8.4658 - val_Id_loss: 8.3593 - val_l2_loss_loss: 0.2659 - val_Id_accuracy: 0.0863 - val_l2_loss_accuracy: 0.0000e+00\n",
      "\n",
      "Epoch 00006: saving model to best_model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1cad8457518>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Yval_dummy = np.random.rand(Xval.shape[0],1)\n",
    "\n",
    "# es is the esrly stop condition\n",
    "es = EarlyStopping(monitor='val_Id_loss', mode='min',patience=5)\n",
    "\n",
    "# the model check point is similar to saving model when meet the condition\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_Id_loss', mode='min', verbose=1)\n",
    "\n",
    "# Train model\n",
    "model.fit_generator(generate_data_generator(generator,Xtrain,Ytrain),steps_per_epoch = 100 ,epochs = 20,validation_data=([Xval,Yval_value], [Yval,Yval_dummy]), callbacks=[es,mc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, one load best weights saved in previous training and evaluate the model using testing data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1198/1198 [==============================] - 18s 15ms/step\n"
     ]
    }
   ],
   "source": [
    "Ytest_dummy = np.random.rand(Xtest.shape[0],1)\n",
    "model.load_weights('best_model.h5')\n",
    "preds_1 = model.evaluate([Xtest,Ytest_value], [Ytest,Ytest_dummy], batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss', 'Id_loss', 'l2_loss_loss', 'Id_accuracy', 'l2_loss_accuracy']\n",
      "[8.479139447411233, 8.366284370422363, 0.27185383439064026, 0.0826377272605896, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics_names)\n",
    "print(preds_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the accurracy and loss are printed out in this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss = 8.366284370422363\n",
      "Test Accuracy = 0.0826377272605896\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Loss = \" + str(preds_1[1]))\n",
    "print(\"Test Accuracy = \" + str(preds_1[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In the conclusion, there are many things to discussed here. First, I would like to start with the data. As one can see, this data set is similar to the face verification problem, which only few data provided and to match with Id. One thing one should keep in mind is that one should do the <strong>pre-processing</strong> before training with these images. In general, I would crop out the non-interesting thing, i.e. detect and crop only the object, using any technique, such as color detection, or even hand-crop it. In some application, one might convert the channel of the image to another channel (HSV, Grayscale, etc.). For this project, I found that the image is quite satisfied after crop the explaination text out. So, I didn't do any detection to select only the whale's tail. Another thing is the <strong>data augmentation</strong> can help with training set when it come to small number of image for each class.</p> \n",
    "\n",
    "<p>The next topic here is the model designing. After did some research, I found that there is another model which might work better for this kind of application. <strong>The Siamese Neural Networks</strong> are well-known for <strong>\"One Shot Learning\"</strong> which have a potential to deliver the better result than the one I trained. I consider this task as \"One shot learning\" since the number of images for each class is clearly considered small. </p>\n",
    "    \n",
    "    \n",
    "<p>The last issue I would like to mentioned here is the computation resource. As you may know, I am running this on my personal laptop and it is not able to deliver such a high performance. With powerful comtutational resource, I might use the bigger batchsize, train the model longer, or use a bigger network, such as VGG19.</p>\n",
    "\n",
    "<p>In summary, I would say that the algorithm in this notebook might not considered as a great model which is giving promising result but it is the good start to truly get familiar with this data set. In my opinion, modelling the model is the task one need to iterate the work as fast as possible, i.e. research, set up and train fast, so one can make a change or moving forward faster. This model is going to be a jump start for better version or any other model, which deliver a good result for this project.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
